"""
LLM å®¢æˆ·ç«¯æ¨¡å— - å°è£…å¤§æ¨¡å‹è°ƒç”¨

åŠŸèƒ½:
- å°è£… OpenAI API è°ƒç”¨
- è¯¦ç»†çš„è¾“å…¥/è¾“å‡ºæ—¥å¿—è®°å½•
- æ”¯æŒ Function Calling
- æ”¯æŒæµå¼è¾“å‡ºï¼ˆStreamingï¼‰
- å®Œæ•´çš„è¯·æ±‚/å“åº” JSON è®°å½•ï¼ˆä¿å­˜åˆ° record æ–‡ä»¶å¤¹ï¼‰
"""
import json
import os
import time
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional, Callable, Awaitable
from openai import OpenAI, AsyncOpenAI

from config.settings import settings
from utils.logger import logger


class LLMClient:
    """å¤§æ¨¡å‹å®¢æˆ·ç«¯å°è£…ï¼ˆå¸¦è¯¦ç»†æ—¥å¿—ï¼Œæ”¯æŒæµå¼è¾“å‡ºï¼‰"""
    
    def __init__(self):
        # åŒæ­¥å®¢æˆ·ç«¯ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
        self.client = OpenAI(
            api_key=settings.LLM_API_KEY,
            base_url=settings.LLM_BASE_URL
        )
        # å¼‚æ­¥å®¢æˆ·ç«¯ï¼ˆç”¨äºæµå¼è¾“å‡ºï¼‰
        self.async_client = AsyncOpenAI(
            api_key=settings.LLM_API_KEY,
            base_url=settings.LLM_BASE_URL
        )
        self.model = settings.LLM_MODEL
        self.call_count = 0
        self.current_session_id = None
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ record æ–‡ä»¶å¤¹è·¯å¾„
        self.record_dir = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
            "record"
        )
        # ç¡®ä¿ record ç›®å½•å­˜åœ¨
        os.makedirs(self.record_dir, exist_ok=True)
        
        # é»˜è®¤æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆä¼šåœ¨ set_session æ—¶æ›´æ–°ï¼‰
        self.session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_file_path = os.path.join(
            self.record_dir, 
            f"llm_log_{self.session_timestamp}.txt"
        )
        
        logger.info(f"[LLM] å®¢æˆ·ç«¯åˆå§‹åŒ–: model={self.model}, base_url={settings.LLM_BASE_URL or 'default'}")
        logger.info(f"[LLM] JSONæ—¥å¿—æ–‡ä»¶: {self.log_file_path}")
        logger.info(f"[LLM] æµå¼è¾“å‡º: å·²å¯ç”¨")
    
    def set_session(self, session_id: str):
        """
        è®¾ç½®å½“å‰ sessionï¼Œæ›´æ–°æ—¥å¿—æ–‡ä»¶è·¯å¾„
        
        æ¯ä¸ª session ä¼šç”Ÿæˆç‹¬ç«‹çš„ llm_log æ–‡ä»¶
        
        Args:
            session_id: ä¼šè¯ ID
        """
        self.current_session_id = session_id
        self.call_count = 0  # é‡ç½®è°ƒç”¨è®¡æ•°
        self.session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä½¿ç”¨ session_id å‰8ä½ + æ—¶é—´æˆ³ ä½œä¸ºæ–‡ä»¶åï¼Œæ–¹ä¾¿å…³è”
        short_session_id = session_id[:8] if len(session_id) >= 8 else session_id
        self.log_file_path = os.path.join(
            self.record_dir, 
            f"llm_log_{short_session_id}_{self.session_timestamp}.txt"
        )
        
        logger.info(f"[LLM] åˆ‡æ¢ Session: {session_id[:8]}...")
        logger.info(f"[LLM] æ–°æ—¥å¿—æ–‡ä»¶: {self.log_file_path}")
    
    def _save_json_log(
        self, 
        request_data: Dict[str, Any], 
        response_data: Dict[str, Any], 
        raw_response: Optional[Any] = None,
        duration: float = 0
    ):
        """
        ä¿å­˜è¯·æ±‚å’Œå“åº”çš„å®Œæ•´ JSON åˆ°æ–‡ä»¶
        
        Args:
            request_data: å‘é€ç»™å¤§æ¨¡å‹çš„è¯·æ±‚æ•°æ®
            response_data: å¤„ç†åçš„å“åº”æ•°æ®
            raw_response: åŸå§‹ API å“åº”å¯¹è±¡
            duration: è¯·æ±‚è€—æ—¶
        """
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
            
            log_entry = {
                "call_number": self.call_count,
                "timestamp": timestamp,
                "duration_seconds": round(duration, 3),
                "request": request_data,
                "response": response_data
            }
            
            # å¦‚æœæœ‰åŸå§‹å“åº”ï¼Œå°è¯•æå– usage ä¿¡æ¯
            if raw_response and hasattr(raw_response, 'usage') and raw_response.usage:
                log_entry["token_usage"] = {
                    "prompt_tokens": raw_response.usage.prompt_tokens,
                    "completion_tokens": raw_response.usage.completion_tokens,
                    "total_tokens": raw_response.usage.total_tokens
                }
            
            # è¿½åŠ å†™å…¥æ—¥å¿—æ–‡ä»¶
            with open(self.log_file_path, 'a', encoding='utf-8') as f:
                f.write(f"\n{'='*80}\n")
                f.write(f"=== LLM è°ƒç”¨ #{self.call_count} - {timestamp} ===\n")
                f.write(f"{'='*80}\n\n")
                f.write(json.dumps(log_entry, ensure_ascii=False, indent=2))
                f.write(f"\n\n")
            
            logger.debug(f"[LLM] JSONæ—¥å¿—å·²ä¿å­˜: è°ƒç”¨ #{self.call_count}")
            
        except Exception as e:
            logger.warning(f"[LLM] ä¿å­˜JSONæ—¥å¿—å¤±è´¥: {e}")
    
    def _log_request(self, messages: List[Dict[str, Any]], tools: Optional[List] = None, extra_params: dict = None):
        """è®°å½•è¯·æ±‚æ—¥å¿—"""
        self.call_count += 1
        
        logger.info(f"\n{'='*60}")
        logger.info(f"[LLM] ===== ç¬¬ {self.call_count} æ¬¡è°ƒç”¨ =====")
        logger.info(f"[LLM] æ¨¡å‹: {self.model}")
        logger.info(f"[LLM] æ¶ˆæ¯æ•°é‡: {len(messages)}")
        
        # è®°å½•æœ€åå‡ æ¡æ¶ˆæ¯ï¼ˆæœ€ç›¸å…³ï¼‰
        logger.info(f"[LLM] --- è¾“å…¥æ¶ˆæ¯ ---")
        for i, msg in enumerate(messages[-3:]):  # åªæ˜¾ç¤ºæœ€å3æ¡
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')
            
            # æˆªæ–­è¿‡é•¿çš„å†…å®¹
            if content and len(str(content)) > 500:
                content = str(content)[:500] + "... (æˆªæ–­)"
            
            if msg.get('tool_calls'):
                logger.info(f"[LLM]   [{i}] role={role}, tool_calls={msg['tool_calls']}")
            elif role == 'tool':
                logger.info(f"[LLM]   [{i}] role={role}, tool_call_id={msg.get('tool_call_id')}")
                logger.info(f"[LLM]       å†…å®¹: {content}")
            else:
                logger.info(f"[LLM]   [{i}] role={role}")
                if content:
                    # å¯¹äºé•¿å†…å®¹ï¼Œåªæ˜¾ç¤ºå‰å‡ è¡Œ
                    lines = str(content).split('\n')[:5]
                    for line in lines:
                        if line.strip():
                            logger.info(f"[LLM]       {line[:100]}")
        
        if tools:
            tool_names = [t.get('function', {}).get('name', 'unknown') for t in tools]
            logger.info(f"[LLM] å¯ç”¨å·¥å…·: {tool_names}")
        
        if extra_params:
            logger.info(f"[LLM] é¢å¤–å‚æ•°: {extra_params}")
    
    def _extract_reasoning(self, message) -> tuple[Optional[str], Optional[str]]:
        """
        ä»æ¨¡å‹å“åº”ä¸­æå–æ€è€ƒè¿‡ç¨‹å’ŒåŸå§‹å­—æ®µå
        
        æ”¯æŒå¤šç§å­—æ®µåï¼ˆä¸åŒæ¨¡å‹å¯èƒ½ä½¿ç”¨ä¸åŒçš„å­—æ®µï¼‰ï¼š
        - reasoning_content: DeepSeek-R1 ç­‰æ¨¡å‹
        - reasoning: é€šç”¨å­—æ®µå
        - thinking_content: æŸäº›æ¨¡å‹
        - thinking: æŸäº›æ¨¡å‹
        - reason: æŸäº›æ¨¡å‹
        
        Returns:
            (reasoning_value, original_field_name) å…ƒç»„ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› (None, None)
        """
        # å¯èƒ½çš„æ€è€ƒè¿‡ç¨‹å­—æ®µååˆ—è¡¨
        reasoning_fields = [
            'reasoning_content',
            'reasoning',
            'thinking_content', 
            'thinking',
            'reason',
            'thought',
            'chain_of_thought'
        ]
        
        # å°è¯•ä» message å¯¹è±¡ä¸­æå–
        for field in reasoning_fields:
            if hasattr(message, field):
                value = getattr(message, field)
                if value:
                    return (str(value), field)
        
        # å°è¯•ä» message çš„ __dict__ ä¸­æå–ï¼ˆæŸäº›æ¨¡å‹å¯èƒ½ä½¿ç”¨åŠ¨æ€å±æ€§ï¼‰
        if hasattr(message, '__dict__'):
            for field in reasoning_fields:
                if field in message.__dict__ and message.__dict__[field]:
                    return (str(message.__dict__[field]), field)
        
        # å°è¯•ä» message ä½œä¸ºå­—å…¸è®¿é—®ï¼ˆæŸäº› API å¯èƒ½è¿”å›å­—å…¸ï¼‰
        if isinstance(message, dict):
            for field in reasoning_fields:
                if field in message and message[field]:
                    return (str(message[field]), field)
        
        return (None, None)
    
    def _log_response(self, response_type: str, result: Dict[str, Any], duration: float):
        """è®°å½•å“åº”æ—¥å¿—"""
        logger.info(f"[LLM] --- è¾“å‡ºå“åº” ---")
        logger.info(f"[LLM] å“åº”ç±»å‹: {response_type}")
        logger.info(f"[LLM] è€—æ—¶: {duration:.2f}ç§’")
        
        if response_type == "tool_call":
            logger.info(f"[LLM] å·¥å…·è°ƒç”¨: {result.get('name')}")
            args = result.get('arguments', {})
            # ç‰¹æ®Šå¤„ç†ä»£ç å‚æ•°
            if 'code' in args:
                code_preview = args['code'][:300] + "..." if len(args['code']) > 300 else args['code']
                logger.info(f"[LLM] å‚æ•°: description={args.get('description', '')}")
                logger.info(f"[LLM] ä»£ç é¢„è§ˆ:\n{code_preview}")
            else:
                logger.info(f"[LLM] å‚æ•°: {json.dumps(args, ensure_ascii=False)[:500]}")
        
        elif response_type == "response":
            content = result.get('content', '')
            if len(str(content)) > 500:
                logger.info(f"[LLM] å†…å®¹é¢„è§ˆ: {str(content)[:500]}... (æˆªæ–­)")
            else:
                logger.info(f"[LLM] å†…å®¹: {content}")
        
        elif response_type == "error":
            logger.error(f"[LLM] é”™è¯¯: {result.get('error')}")
        
        logger.info(f"{'='*60}\n")
    
    def chat(
        self,
        messages: List[Dict[str, Any]],
        tools: Optional[List[Dict[str, Any]]] = None,
        temperature: float = 0.7,
        max_tokens: int = 4096
    ) -> Dict[str, Any]:
        """
        å‘é€èŠå¤©è¯·æ±‚
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
        
        Returns:
            åŒ…å«å“åº”ç±»å‹å’Œå†…å®¹çš„å­—å…¸
        """
        # è®°å½•è¯·æ±‚
        self._log_request(messages, tools, {"temperature": temperature, "max_tokens": max_tokens})
        
        start_time = time.time()
        
        # æ„å»ºè¯·æ±‚æ•°æ®ç”¨äºæ—¥å¿—
        request_data = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        if tools:
            request_data["tools"] = tools
            request_data["tool_choice"] = "auto"
        
        try:
            kwargs = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            if tools:
                kwargs["tools"] = tools
                kwargs["tool_choice"] = "auto"
            
            response = self.client.chat.completions.create(**kwargs)
            
            duration = time.time() - start_time
            message = response.choices[0].message
            
            # è®°å½• token ä½¿ç”¨æƒ…å†µ
            if hasattr(response, 'usage') and response.usage:
                logger.info(f"[LLM] Token ä½¿ç”¨: prompt={response.usage.prompt_tokens}, completion={response.usage.completion_tokens}, total={response.usage.total_tokens}")
            
            # æå–æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹å’ŒåŸå§‹å­—æ®µå
            reasoning, reasoning_field_name = self._extract_reasoning(message)
            if reasoning:
                logger.info(f"[LLM] ğŸ§  æ¨¡å‹æ€è€ƒè¿‡ç¨‹: {reasoning[:200]}...")
            
            # æ„å»ºåŸå§‹å“åº”æ•°æ®ç”¨äºæ—¥å¿—ï¼ˆä¿ç•™åŸå§‹å­—æ®µåï¼‰
            message_dict = {
                "role": message.role,
                "content": message.content
            }
            # å¦‚æœæœ‰æ€è€ƒè¿‡ç¨‹ï¼Œä½¿ç”¨åŸå§‹å­—æ®µå
            if reasoning and reasoning_field_name:
                message_dict[reasoning_field_name] = reasoning
            
            raw_response_data = {
                "id": response.id if hasattr(response, 'id') else None,
                "model": response.model if hasattr(response, 'model') else None,
                "choices": [{
                    "index": response.choices[0].index if hasattr(response.choices[0], 'index') else 0,
                    "message": message_dict,
                    "finish_reason": response.choices[0].finish_reason if hasattr(response.choices[0], 'finish_reason') else None
                }]
            }
            
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ åˆ°å“åº”æ•°æ®
            if message.tool_calls:
                raw_response_data["choices"][0]["message"]["tool_calls"] = [
                    {
                        "id": tc.id,
                        "type": tc.type,
                        "function": {
                            "name": tc.function.name,
                            "arguments": tc.function.arguments
                        }
                    } for tc in message.tool_calls
                ]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if message.tool_calls:
                tool_call = message.tool_calls[0]
                result = {
                    "type": "tool_call",
                    "tool_call_id": tool_call.id,
                    "name": tool_call.function.name,
                    "arguments": json.loads(tool_call.function.arguments),
                    "content": message.content or "",  # ä¿ç•™æ–‡æœ¬å†…å®¹
                    "reasoning": reasoning  # æ·»åŠ æ€è€ƒè¿‡ç¨‹
                }
                self._log_response("tool_call", result, duration)
                
                # ä¿å­˜ JSON æ—¥å¿—
                self._save_json_log(request_data, raw_response_data, response, duration)
                
                return result
            
            # æ™®é€šæ–‡æœ¬å“åº”
            result = {
                "type": "response",
                "content": message.content or "",
                "reasoning": reasoning  # æ·»åŠ æ€è€ƒè¿‡ç¨‹
            }
            self._log_response("response", result, duration)
            
            # ä¿å­˜ JSON æ—¥å¿—
            self._save_json_log(request_data, raw_response_data, response, duration)
            
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            result = {
                "type": "error",
                "error": str(e)
            }
            self._log_response("error", result, duration)
            
            # ä¿å­˜é”™è¯¯æ—¥å¿—
            self._save_json_log(request_data, {"error": str(e), "type": "error"}, None, duration)
            
            return result
    
    async def chat_stream(
        self,
        messages: List[Dict[str, Any]],
        tools: Optional[List[Dict[str, Any]]] = None,
        on_content_chunk: Optional[Callable[[str], Awaitable[None]]] = None,
        on_reasoning_chunk: Optional[Callable[[str], Awaitable[None]]] = None,
        on_tool_call_start: Optional[Callable[[str], Awaitable[None]]] = None,
        temperature: float = 0.7,
        max_tokens: int = 4096
    ) -> Dict[str, Any]:
        """
        å¼‚æ­¥æµå¼èŠå¤©è¯·æ±‚
        
        æ”¯æŒåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®æ—¶å›è°ƒï¼Œå®ç°æ‰“å­—æœºæ•ˆæœ
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨
            on_content_chunk: å†…å®¹å—å›è°ƒï¼ˆæ¯ç”Ÿæˆä¸€å°æ®µæ–‡æœ¬å°±è°ƒç”¨ï¼‰
            on_reasoning_chunk: æ€è€ƒè¿‡ç¨‹å—å›è°ƒï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
            on_tool_call_start: å·¥å…·è°ƒç”¨å¼€å§‹å›è°ƒ
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
        
        Returns:
            åŒ…å«å“åº”ç±»å‹å’Œå†…å®¹çš„å­—å…¸
        """
        # è®°å½•è¯·æ±‚
        self._log_request(messages, tools, {"temperature": temperature, "max_tokens": max_tokens, "stream": True})
        
        start_time = time.time()
        
        # æ„å»ºè¯·æ±‚æ•°æ®ç”¨äºæ—¥å¿—
        request_data = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": True
        }
        if tools:
            request_data["tools"] = tools
            request_data["tool_choice"] = "auto"
        
        try:
            kwargs = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": True
            }
            
            if tools:
                kwargs["tools"] = tools
                kwargs["tool_choice"] = "auto"
            
            # ä½¿ç”¨å¼‚æ­¥å®¢æˆ·ç«¯è¿›è¡Œæµå¼è°ƒç”¨
            stream = await self.async_client.chat.completions.create(**kwargs)
            
            # æ”¶é›†å®Œæ•´å“åº”
            full_content = ""
            full_reasoning = ""
            reasoning_field_name = None  # è®°å½•åŸå§‹å­—æ®µå
            tool_calls_data: Dict[int, Dict[str, Any]] = {}  # index -> {id, name, arguments}
            finish_reason = None
            
            # å¤„ç†æµå¼å“åº”
            async for chunk in stream:
                if not chunk.choices:
                    continue
                    
                choice = chunk.choices[0]
                delta = choice.delta
                
                # è®°å½•ç»“æŸåŸå› 
                if choice.finish_reason:
                    finish_reason = choice.finish_reason
                
                # å¤„ç†æ€è€ƒè¿‡ç¨‹ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼Œå¦‚ DeepSeek-R1ï¼‰
                # ä¼˜å…ˆä½¿ç”¨ reasoning_contentï¼ˆKimi thinking æ¨¡å‹å®˜æ–¹å­—æ®µï¼‰
                reasoning_content = None
                if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                    reasoning_content = delta.reasoning_content
                    if reasoning_field_name is None:
                        reasoning_field_name = 'reasoning_content'
                elif hasattr(delta, 'reasoning') and delta.reasoning:
                    reasoning_content = delta.reasoning
                    if reasoning_field_name is None:
                        reasoning_field_name = 'reasoning'
                
                if reasoning_content:
                    full_reasoning += reasoning_content
                    if on_reasoning_chunk:
                        await on_reasoning_chunk(reasoning_content)
                
                # å¤„ç†æ–‡æœ¬å†…å®¹
                if delta.content:
                    full_content += delta.content
                    if on_content_chunk:
                        await on_content_chunk(delta.content)
                
                # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆæµå¼ä¸­éœ€è¦æ‹¼æ¥ï¼‰
                if delta.tool_calls:
                    for tc in delta.tool_calls:
                        idx = tc.index
                        
                        if idx not in tool_calls_data:
                            tool_calls_data[idx] = {
                                "id": "",
                                "name": "",
                                "arguments": ""
                            }
                        
                        if tc.id:
                            tool_calls_data[idx]["id"] = tc.id
                        
                        if tc.function:
                            if tc.function.name:
                                tool_calls_data[idx]["name"] = tc.function.name
                                # é€šçŸ¥å·¥å…·è°ƒç”¨å¼€å§‹
                                if on_tool_call_start:
                                    await on_tool_call_start(tc.function.name)
                            
                            if tc.function.arguments:
                                tool_calls_data[idx]["arguments"] += tc.function.arguments
            
            duration = time.time() - start_time
            
            # è®°å½• token ä½¿ç”¨ï¼ˆæµå¼æ¨¡å¼ä¸‹å¯èƒ½æ²¡æœ‰ï¼‰
            logger.info(f"[LLM] æµå¼å“åº”å®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
            
            # æ„å»ºå“åº”æ•°æ®ç”¨äºæ—¥å¿—ï¼ˆä¿ç•™åŸå§‹å­—æ®µåï¼‰
            message_dict = {
                "role": "assistant",
                "content": full_content
            }
            # å¦‚æœæœ‰æ€è€ƒè¿‡ç¨‹ï¼Œä½¿ç”¨åŸå§‹å­—æ®µå
            if full_reasoning and reasoning_field_name:
                message_dict[reasoning_field_name] = full_reasoning
            
            raw_response_data = {
                "model": self.model,
                "stream": True,
                "choices": [{
                    "message": message_dict,
                    "finish_reason": finish_reason
                }]
            }
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if tool_calls_data:
                # å–ç¬¬ä¸€ä¸ªå·¥å…·è°ƒç”¨
                first_tool = tool_calls_data[0]
                
                raw_response_data["choices"][0]["message"]["tool_calls"] = [
                    {
                        "id": tc_data["id"],
                        "type": "function",
                        "function": {
                            "name": tc_data["name"],
                            "arguments": tc_data["arguments"]
                        }
                    } for tc_data in tool_calls_data.values()
                ]
                
                try:
                    arguments = json.loads(first_tool["arguments"])
                except json.JSONDecodeError:
                    arguments = {}
                
                result = {
                    "type": "tool_call",
                    "tool_call_id": first_tool["id"],
                    "name": first_tool["name"],
                    "arguments": arguments,
                    "content": full_content,
                    "reasoning": full_reasoning if full_reasoning else None
                }
                
                self._log_response("tool_call", result, duration)
                self._save_json_log(request_data, raw_response_data, None, duration)
                
                return result
            
            # æ™®é€šæ–‡æœ¬å“åº”
            result = {
                "type": "response",
                "content": full_content,
                "reasoning": full_reasoning if full_reasoning else None
            }
            
            self._log_response("response", result, duration)
            self._save_json_log(request_data, raw_response_data, None, duration)
            
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            result = {
                "type": "error",
                "error": str(e)
            }
            self._log_response("error", result, duration)
            self._save_json_log(request_data, {"error": str(e), "type": "error"}, None, duration)
            
            return result
    
    def chat_json(
        self,
        messages: List[Dict[str, Any]],
        temperature: float = 0.3
    ) -> Dict[str, Any]:
        """
        å‘é€è¯·æ±‚å¹¶æœŸæœ› JSON å“åº”
        """
        # è®°å½•è¯·æ±‚
        self._log_request(messages, None, {"temperature": temperature, "response_format": "json_object"})
        
        start_time = time.time()
        
        # æ„å»ºè¯·æ±‚æ•°æ®ç”¨äºæ—¥å¿—
        request_data = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "response_format": {"type": "json_object"}
        }
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                response_format={"type": "json_object"}
            )
            
            duration = time.time() - start_time
            content = response.choices[0].message.content
            
            # è®°å½• token ä½¿ç”¨æƒ…å†µ
            if hasattr(response, 'usage') and response.usage:
                logger.info(f"[LLM] Token ä½¿ç”¨: prompt={response.usage.prompt_tokens}, completion={response.usage.completion_tokens}, total={response.usage.total_tokens}")
            
            # æ„å»ºåŸå§‹å“åº”æ•°æ®ç”¨äºæ—¥å¿—
            raw_response_data = {
                "id": response.id if hasattr(response, 'id') else None,
                "model": response.model if hasattr(response, 'model') else None,
                "choices": [{
                    "index": response.choices[0].index if hasattr(response.choices[0], 'index') else 0,
                    "message": {
                        "role": response.choices[0].message.role,
                        "content": content
                    },
                    "finish_reason": response.choices[0].finish_reason if hasattr(response.choices[0], 'finish_reason') else None
                }]
            }
            
            result = {
                "type": "response",
                "content": json.loads(content)
            }
            
            # è®°å½•å“åº”
            logger.info(f"[LLM] --- JSON å“åº” ---")
            logger.info(f"[LLM] è€—æ—¶: {duration:.2f}ç§’")
            logger.info(f"[LLM] JSON å†…å®¹é¢„è§ˆ: {json.dumps(result['content'], ensure_ascii=False)[:500]}")
            logger.info(f"{'='*60}\n")
            
            # ä¿å­˜ JSON æ—¥å¿—
            self._save_json_log(request_data, raw_response_data, response, duration)
            
            return result
            
        except json.JSONDecodeError as e:
            duration = time.time() - start_time
            result = {
                "type": "error",
                "error": f"JSON è§£æé”™è¯¯: {str(e)}"
            }
            self._log_response("error", result, duration)
            
            # ä¿å­˜é”™è¯¯æ—¥å¿—
            self._save_json_log(request_data, {"error": str(e), "type": "json_decode_error"}, None, duration)
            
            return result
        except Exception as e:
            duration = time.time() - start_time
            result = {
                "type": "error",
                "error": str(e)
            }
            self._log_response("error", result, duration)
            
            # ä¿å­˜é”™è¯¯æ—¥å¿—
            self._save_json_log(request_data, {"error": str(e), "type": "error"}, None, duration)
            
            return result


# å…¨å±€ LLM å®¢æˆ·ç«¯å®ä¾‹
_llm_client: Optional[LLMClient] = None


def get_llm_client() -> LLMClient:
    """è·å– LLM å®¢æˆ·ç«¯å•ä¾‹"""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client

